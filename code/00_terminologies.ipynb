{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style = \"font-weight : bold; font-size : 24px; color : black \">Terminologies<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: solid rgb(255,255,255) 1.0px;height: 2.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style = \"font-weight : bold; font-size : 18px; color : black \">Decision Trees<font>\n",
    "    \n",
    "    \n",
    "<font style = \"font-weight : bold; font-size : 14px; color : blue \"> URL <font> \n",
    "    \n",
    "    https://www.youtube.com/watch?v=7VeUPuFGJHk&t=475s\n",
    "    \n",
    "<font style = \"font-weight : bold; font-size : 14px; color : blue \">Points to Remember<font>\n",
    "    \n",
    "        1. A Decision Tree can have repetitive leaf nodes.\n",
    "    \n",
    "        2. Root Node : Where the tree starts\n",
    "           Internal Node : Where node arrow point-in and points-out.\n",
    "           Leaf Node : Where node arrow points-in only.\n",
    "          \n",
    "        3. Impure : If none of the leaf node predicts 100% classify a lable than all the leaf node are consider \"Impure. An \n",
    "        \n",
    "        Example would be, \n",
    "        \n",
    "        If appearing for an exam and learning from DP, some student may Pass some may fail. Than Learning \n",
    "        from DP >> will have Pass for some as Yes and some for No. This is an IMPURE leaf node.\n",
    "        \n",
    "        Lying down on a moving road >> Chances of accidents >> all data rows will say yes. Hence this data \n",
    "        set would pe PURE. \n",
    "        \n",
    "        4. Gini (read Jini) Index : Measurement for impurity\n",
    "        \n",
    "        5. Identify Root Node : Feature that has lowest impurity (Gini Index) is considered as root node. Subsequently, \n",
    "        internal nodes are identify with next-best Gini Index and so no. If the impurity is lower than the Gini Index of \n",
    "        feature than we make it as leaf node. (video section : 12:30 min to 14 min)\n",
    "        \n",
    "        \n",
    "        6. Decision tools are easy to interpret & build. But they are widely known for making inaccurate prediction. A quote \n",
    "        from \"The Elements of Statistical Learning\" aka the bible of ML, \"Decision Trees have one aspect that prevents them \n",
    "        from being the ideal tool for predicutive learning, namely inaccuracy\".\n",
    "        In other words, they work great with the data used to create them, but they are not flexible when it comes to \n",
    "        classifying new samples.\n",
    "        The alternative to Decision Tress is Random Forest.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style = \"font-weight : bold; font-size : 18px; color : black \">Random Forest<font>\n",
    "    \n",
    "    \n",
    "<font style = \"font-weight : bold; font-size : 14px; color : blue \"> URL <font> \n",
    "    \n",
    "    https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\n",
    "    \n",
    "<font style = \"font-weight : bold; font-size : 14px; color : blue \">Points to Remember<font>\n",
    "    \n",
    "    1. Bootstraped Dataset : \n",
    "        a. It starts with randomly selecting rows from the initial dataset. Point to note is same row is allowed to be randomly \n",
    "        selected more than once.\n",
    "        b. Built a decision tree using a random subset of variables.\n",
    "        c. Repeat step 1 & 2 for hundreds of time with randly selected rows (samples).\n",
    "        \n",
    "    2. Bagging : Bootstraping the data and using aggregate for new sample to make a decision is called Bagging. Here aggregate \n",
    "    means that we have a got a new sample, we run the new sample against all trees that are built using Bootstraping. Now if \n",
    "    majorirty of trees says \"Yes\" or classifies the new data in one class than that majority is considered as the prediction \n",
    "    label.\n",
    "    \n",
    "    3. Out-of-Bag Dataset : The samples that were randomly left out at the start of sampling (bootstraping) are used to \n",
    "    determine the correctness of any Random Forest.\n",
    "    \n",
    "    4. Out-of-Bag Error : The proportion of samples that were incorrectly identified by a forest is called as Out-of-Bag Error.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
